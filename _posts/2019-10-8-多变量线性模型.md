---
layout: post
title: 多变量线性模型
category: 日志
keywords: 机器学习,线性模型
---

by 田鸿龙

我们之前已经学会了处理一元线性回归问题，接下来让我们继续探索，考虑多元问题是如何解决的。


首先，我将给你多变量回归的线性代数表述，接下来，我会试着用线性代数的语言描述这种方法。


相信我，尽管这样的问题看起来已经很有技术含量了，但聪明的你依旧可以很快理解。

## 问题求解


假设我们面临这样一个问题：你有一栋坐落于环境优美的田园的别墅，但是由于你最近想体验四海为家的流浪生活，买了一只游轮打算在太平洋漂泊。所以，为了不让你的别墅变成小动物的乐园，你打算近期把它以合适的价格卖出去。

![img](https://raw.githubusercontent.com/nju-se-ai-group/nju-se-ai-group.github.io/master/img/post/房子.jpeg)




那么问题来了，别墅的定价为多少才算合适呢？


毕竟别墅是一件奢侈品，你不想简单的用每平米价格乘上面积这样没有技术含量的方式给你的别墅定价。


你打算用机器学习的方法实现这个过程。


我们上一章已经提到了，想训练一个机器学习算法，前提是有一些收集好的数据。如果是监督学习，那不光要有属性值，还要有对应的标记。


所以你首先要做的是收集已经完成交易的别墅的记录。


假设你已经做了足够充分的市场调查，并列出了如下的清单：

1. 面积
2. 楼层数
3. 卧室数目
4. 卫生间数目
5. 车库数目
6. 离市中心的距离
7. 离最近的商场的距离
8. 离最近的公园的距离


当然，如果你愿意的话，你可以写的更多，实际上，哪怕你一口气写下了几万条这样的信息（尽管我猜你没那么无聊），对于现在的计算机而言也都是小case了。


在机器学习中，我们称上面列出的八条信息为**属性**，称这些属性可能的取值为**属性值**。


如果你通过市场调查得到了一千个已经交易过的别墅的信息，我们称这些信息的集合为**数据集**，看起来这样的命名还是蛮有道理的，对吧。


让我拿刚才的例子进一步解释。


如果你最先收集的信息是你朋友tony的别墅的信息，那这个别墅的信息就是数据集的第一条数据。


数据的第一个属性是面积，如果你的朋友tony偏爱素数，那么他别墅的面积也许是647平方米，更精确一点，也许是647.823平方米。


第二个属性是楼层数，显然，如果你个朋友tony是个麻瓜的话，他不会允许自己的别墅出现三又二分之一层这样奇怪的数字。我们暂且假定tony的别墅是3层的。

如果你乐此不疲的话，你可以继续给tony的别墅的所有属性都编上不同的数字。但是别忘了，最后一定也要编上别墅最后**成交的价格**，因为我们在考虑一个**监督学习**问题。



## 离散属性与连续属性


同时，我猜你已经注意到了，在上面的数据中，有的属性取值可以很随意（在一定范围内的随意），有的却只能取几个（只要不是无数个）固定的值。


我们称那些可以随意取值的属性是**连续**的，例如别墅的面积。


称那些只有几个固定取值的属性是**离散**的，例如别墅的层数。


如果tony是霍格沃茨的优秀毕业生，他允许自己的别墅出现三又二分之一层这样的数字，但“别墅的层数”这个属性依旧是离散的。

我想你明白我的意思，离散的属性值不一定是整数，只要可能的取值不是无限个，它就是离散的。



而别墅的面积这样的属性，哪怕只能在600到601之间取值，你也能列出无限个值，对吧？所以它是连续的。



实际上，如果我们只有一个属性，比如说面积，那我们已经知道该怎么做了。


我们之前的假设函数是这样的：

$$
\hat y = b \hat x+a
$$
现在我们不满足于一个x了，我们要把更多的自变量写进这个方程。

## 多元线性模型


也许你会纠结于我刚才所说的离散和连续的区别，这确实是我们需要考虑的因素，但我并不打算在这章讨论它们。你可以拿个小本子先记下来，如果我忘记了请及时提醒我。


我们改写一下假设函数（在这里我用字母h代表假设函数，如果你知道“假设”的英文，你就知道我为什么怎么做了）：

$$
h(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8) = \sum_{i=1}^{n}\theta_ix_i + \theta_0
$$


虽然我用了不同的字母，但你应该能看出来这和我们之前一元线性回归讨论的假设函数其实是一回事。另外，有时候我们也用$w$代替$\theta$.


因为多元线性回归的自变量数目不确定，我们不会再用字母指代它们，而是采用了$x$加下角标的形式。同时，之前的系数a和b也消失不见了，取而代之的是$\theta$加下角标的形式，$\theta_0$没有和任何自变量相乘，显然，它替代了之前的b。


如果你之前阅读过统计学相关的书籍，你应该已经对这样的写法见怪不怪了。


同时，值得注意的是f(x1,x2,...,x8)这样的写法实在有点不够简洁，因此，人们更倾向于用矩阵的形式描述这些自变量。


别着急，我会在下面的文章中讲到矩阵的，在这之前，让我们先来了解一下到底怎么求出这些θ的值。


我们还是要先写出代价函数，然后让代价函数的值最小。


如果你对代价函数这个名词有点陌生，没关系，你可以偷偷回到上一章复习一下。我想再过几天你就会完全熟悉这个套路了。


新的代价函数是这样的：



$$
J(\theta_1,\theta_2,\theta_3,\theta_4,\theta_5,\theta_6,\theta_7,\theta_8) = \frac{\sum_{j=1}^{m}(y_i - \sum_{i=1}^{n}\theta_ix_i- \theta_0)^2}{m}
$$



也许你被这个看起来有些复杂的函数吓了一跳，实际上。它一点都不复杂，它和之前一元线性回归的代价函数有异曲同工之妙。


值得注意的是，这里有一些字母意义的变化，在多元线性回归中，我们用m代指数据集的容量，比如你搜集到了一百个别墅的信息，那这里的m就是100。我们用n代指不同的属性，比如x1指别墅的面积，θ1指面积这个属性对应的参数。



用同样的方法，我们需要对不同的θ求偏导（这和我们之前对a和b求偏导的意义相同）：

$$
\frac {\partial J}{\partial \theta_1} = -2\frac{\sum_{j=1}^{m}(y_i - \sum_{i=1}^{n}\theta_ix_i- \theta_0)x_1}{m}
$$

$$
\frac {\partial J}{\partial \theta_2} = -2\frac{\sum_{j=1}^{m}(y_i - \sum_{i=1}^{n}\theta_ix_i- \theta_0)x_2}{m}
$$

$$
...
$$

$$
\frac {\partial J}{\partial \theta_8} = -2\frac{\sum_{j=1}^{m}(y_i - \sum_{i=1}^{n}\theta_ix_i- \theta_0)x_8}{m}
$$

$$
\frac {\partial J}{\partial \theta_0} = -2\frac{\sum_{j=1}^{m}(y_i - \sum_{i=1}^{n}\theta_ix_i- \theta_0)}{m}
$$




让这些偏导数的值为零，我们就可以得到最后的θ的值，将求得的θ代入假设函数，我们就完成了多元线性回归。


值得一提的是，如果我们的数据集包含m个属性，那么我们会得到m+1个不同的θ，分别是$\theta_1$,$\theta_2$,$...$,$\theta_m$，对这些$\theta$分别求偏导，我们可以得到m+1个m+1元一次方程。



m+1个方程，m+1个未知量，我猜你知道这个方程组是**可解**的。



至此，我已经讲完了线性回归的主要内容。

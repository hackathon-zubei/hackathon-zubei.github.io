---
layout: post
title: 单变量线性模型
category: 日志
keywords: 机器学习,线性模型
---
by 田鸿龙

我们先来说那个高中就学过的线性回归是怎么一回事。

回忆一下，高中学线性回归的时候学过一个东西叫回归直线方程，它的公式长成这个样子：



$$
\hat y = b \hat x+a
$$





接下来还有两个公式用来求a和b：

$$
b= \frac{\sum_{i=1}^{n}x_iy_i-n\bar x\bar y}{\sum_{i=1}^{n}x_i^2-n\bar x^2}
$$

$$
a = \bar y- b\bar x
$$







言归正传，如果你只上过高中而没学过微积分的话，你或许熟悉这些公式，但不知道它是怎么来的。


实际上，它是通过对二元函数就偏导都得到的全局最优解，因为这个解是一个式子（我们把式子中的数换成实际问题中的数就得到了我们想要的答案），我们要把这种解叫做闭式解或者解析解。





接下来我来简单的介绍一下偏导，如果你已经学习过（不管你是在小学还是大学学到的），你可以尽情的往下拉，直到看到了你不懂的地方再停止。

## 多元函数与偏导


首先来讲一下多元函数。


多元函数和你之前学过的函数没有什么本质的区别，只不过多元函数受到了多个变量的影响。


举个例子，你之前学过的函数可能长成这样：



$$
f(x)=x^3 + 3x^2+2x+6
$$



或者这样：



$$
f(x) = e^3x+ \ln 2x + \sin \pi x
$$



现在多元函数长成这样：



$$
f(x,y) = x^2+y^2+x+y+1
$$



或者这样：

$$
f(x_1,x_2,x_3) = e^{x_1}\ln ex_2+x_3+10
$$





看起来区别不是很大是吧。


与之前的区别无非是，多元函数的值可能随着多个**自变量的变化**而变化，而不是只受单独一个自变量的影响。


那偏导是咋回事？


我们之前说的导数，反映了函数的变化率，对吧？


那偏导则是反应了函数关于某个自变量的变化率，我们在将一元函数导数的时候没有强调自变量的事，是因为一元函数只有一个自变量。


我猜聪明的你已经想到了怎么求多元函数的偏导了。


没错，就是把要求偏导的自变量当成原来的x，把其他自变量当作常数求导。


比如说我们要对之前的f(x,y)中的x求偏导，我们有时也会说成求f(x,y)对于x的偏导，那么我们只要把x当成自变量，y当成常量就好了：


$$
f(x,y) = x^2+y^2+x+y+1
$$

$$
\frac{\partial f}{\partial x} = 2x+1
$$




$\partial$ 读作partial。


对比我给你的结果和上面的函数，你也许能用我们上一章提到的“无监督学习”的方法摸索出求偏导数的规律。

另外，值得一提的是，接下来我们关于一元导数的写法也可能有所不同：
$$
\frac{df}{dx} = f'(x)
$$



你可能已经见过这种写法了。


另外，关于偏导数，也有以下不同的写法，你不用全部掌握，我也不会换来换去，所以你只要简单了解就好：

$$
\frac{\partial f}{\partial x} \, or \, f'_x \, or \, f_x \, or f'_1
$$


## 回归方程


说完了偏导，我们来说说回归方程到底是怎么回事。


我们先来回忆一下线性回归试图解决的问题：

![img](https://mmbiz.qpic.cn/mmbiz_png/ar0C8brVWNltoAUueuvKqFjTq745ha3ZIYicrzic8bXFZRNjTju2v2g8GM2k5ooMfpH7QOR3gic6PNXnuuhREuheg/640?wx_fmt=png)


这是一道高考题，对吧。


所谓的线性回归问题，就是试图根据已有的数据，总结出一条直线，当你得到了一个新的数据，可以通过这条直线预测出它对应的值。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/ar0C8brVWNltoAUueuvKqFjTq745ha3ZGQjX8X4hvbTWNBg7Lwc0VSf6vBocyWNMtHQqQGSwwlibjiaBAxAm24zg/640?wx_fmt=jpeg)


上面的图中，蓝色的点表示我们已有的数据，绿色的直线就是我们计算出的回归方程。


那么，如何通过一些有限的数据计算出这条直线呢？

## 最小二乘法


实际上，我们是通过最小二乘法实现的。


还记得我们的这个函数

$$
\hat y = b\hat x + a
$$





它就是我们最后要求的直线方程，我们要做的事情就是求出具体的a和b。



在机器学习中，我们把这样的方程成为**“假设函数”**，意思就是假设数据之间存在着这样的关系。


同时，我们把x称为**属性值**，在一元线性回归问题中，我们只有一个属性，所以我不会去强调是哪个属性的属性值。我们将y称为**标记**，显然，只有在**监督学习**的问题才会有标记。


最小二乘法的想法是，计算出每组数据的标记与属性值的假设的差的平方和，并求出使之最小的一组a和b。


尽管上面的表述是我给出的，但我并不是十分满意，因为你可能不是那么容易理解。


相对而言，写成公式要简洁的多：

$$
J(a,b) = \frac {\sum_{i=1}^{n}(y_i-bx_i-a)^2}{n}
$$





注意，这里的$J$叫做代价函数。


在这个简化了的一元线性回归问题中，我们可以用以前代数的形式写出它的自变量。


你可能已经注意到了，因为这个函数的所有x和y都是给定的，所以唯一能改变的量只有a和b，所以这是一个关于a和b的函数。







别忘了，我们的目标是求让cost函数值最小的a和b，聪明的你一定想到了高中求极值的方法。


没错，我们可以对$J$函数求导，通过让导函数为零的方式求得cost的极小值，同时，可以用数学的方法证明，这里的极小值就是最小值。



这里的证明需要用到一些微积分的知识，我就直接跳过了，如果你感兴趣的话可以自行尝试一下，并不是很难。



同时，注意到这是一个二元函数，我们要做的是对a和b分别求偏导，让a和b的偏导都为零，接下来我们就可以得到这个函数的最小值。


我同样省略了这种做法的数学证明。


总之，你能得到这样的偏导数：

$$
\frac {\partial J}{\partial a} = -2\frac{\sum _{i=1}^{n}(y_i-bx_i-a)}{n}
$$



$$
\frac {\partial J}{\partial b} = -2\frac{\sum _{i=1}^{n}(y_i-bx_i-a)x_i}{n}
$$



让偏导为零：

$$
\frac {\partial J}{\partial a} = 0 \,and \,\frac {\partial J}{\partial b} = 0
$$





最后，稍微改变一下形式，我们就可以得到：

$$
\sum _{i=1}^{n}y_i =nb\sum_{i=1}^{n}x_i+na 
$$

$$
\sum _{i=1}^{n}x_iy_i =nb\sum_{i=1}^{n}x_i^2+na \sum_{i=1}^{n}x_i
$$




通过高斯消元法，我们得到了a和b的闭式解：

$$
b= \frac{\sum_{i=1}^{n}x_iy_i-n\bar x\bar y}{\sum_{i=1}^{n}x_i^2-n\bar x^2}
$$

$$
a = \bar y- b\bar x
$$




这就是最简单的一元线性回归问题。

现在你已经得到这条直线的解析式了，接下来，你要做的只是随便拿过来一组数据的属性值，把它当作方程的x代入这个式子，就可以马上得到结果。